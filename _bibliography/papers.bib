---
---

@string{aps = {American Physical Society,}}

@inproceedings{kaliosis-etal-2024-data,
    abbr={ACL 2024},
    title = {A Data-Driven Guided Decoding Mechanism for Diagnostic Captioning},
    author = {Kaliosis, Panagiotis  and Pavlopoulos, John  and
      Charalampakos, Foivos  and
      Moschovis, Georgios  and
      Androutsopoulos, Ion},
    booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
    month = {August},
    year = {2024},
    location = {Bangkok, Thailand},
    publisher = "Association for Computational Linguistics",
    url={https://aclanthology.org/2024.findings-acl.444/},
    doi={10.18653/v1/2024.findings-acl.444},
    pages = {7450--7466},
    google_scholar_id={UeHWp8X0CEIC},
    selected={true},
    annotation={Work done while a member of AUEB and Archimedes AI Research Hub},
    pdf={dmmcs.pdf},
    additional_info={. Our code is available [here](https://github.com/nlpaueb/dmmcs)},
    abstract={Diagnostic Captioning (DC) automatically generates a diagnostic text from one or more medical images (e.g., X-rays, MRIs) of a patient. Treated as a draft, the generated text may assist clinicians, by providing an initial estimation of the patient's condition, speeding up and helping safeguard the diagnostic process. The accuracy of a diagnostic text, however, strongly depends on how well the key medical conditions depicted in the images are expressed. We propose a new data-driven guided decoding method that incorporates medical information, in the form of existing tags capturing key conditions of the image(s), into the beam search of the diagnostic text generation process. We evaluate the proposed method on two medical datasets using four DC systems that range from generic image-to-text systems with CNN encoders and RNN decoders to pre-trained Large Language Models. The latter can also be used in few- and zero-shot learning scenarios. In most cases, the proposed mechanism improves performance with respect to all evaluation measures. We provide an open-source implementation of the proposed method at https://github.com/nlpaueb/dmmcs.}
}

@inproceedings{10627012,
  abbr={ICASSP 2024},
  author={Kaliosis, Panagiotis and Eleftheriou, Sofia and Nikou, Christos and Giannakopoulos, Theodoros},
  booktitle={2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)}, 
  title={A Self-Supervised Learning Approach for Detecting Non-Psychotic Relapses Using Wearable-Based Digital Phenotyping}, 
  year={2024},
  volume={},
  number={},
  pages={97-98},
  url={https://ieeexplore.ieee.org/abstract/document/10627012},
  doi={10.1109/ICASSPW62465.2024.10627012},
  abstract={We present MagCIL’s approach for the 1st track of the "2nd e-Prevention challenge: Psychotic and Non-Psychotic Relapse Detection using Wearable-Based Digital Phenotyping". First we present our approach for preprocessing and extracting features from the wearable’s raw data. We then propose a Transformer model for learning self-supervised representations from augmented features, trained on data from non-relapse days from each of the 9 patients of the challenge. We adopt two unsupervised methods for detecting relapse days as outliers. A separate unsupervised model is tuned for each patient using the validation data of the challenge. Our method ranked second with ROCAUC = 0.651 and PRAUC = 0.642 on the final test dataset of the challenge. The respective code is available at this repository.},
  location={Seoul, South Korea},
  month={April},
  google_scholar_id={zYLM7Y9cAGgC},
  selected={true},
  annotation={Work done while a member of the Computational Intelligence Laboratory of NCSR Demokritos},
  pdf={icasspw24.pdf},
  additional_info={. Our code is available [here](https://github.com/magcil/e-prevention-challenge)}
  }

  @inproceedings{10.1145/3688671.3688771,
    author = {Gkritzali, Evangelia and Kaliosis, Panagiotis and Galanaki, Sofia and Palogiannidi, Elisavet and Giannakopoulos, Theodoros},
    title = {Greek2MathTex: A Greek Speech-to-Text Framework for LaTeX Equations Generation},
    year = {2024},
    isbn = {9798400709821},
    booktitle={Proceedings of the 13th Hellenic Conference on Artificial Intelligence},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3688671.3688771},
    doi = {10.1145/3688671.3688771},
    abstract = {In the vast majority of the academic and scientific domains, LaTeX has established itself as the de facto standard for typesetting complex mathematical equations and formulae. However, LaTeX \'{s} complex syntax and code-like appearance present accessibility barriers for individuals with disabilities, as well as those unfamiliar with coding conventions. In this paper, we present a novel solution to this challenge through the development of a novel speech-to- LaTeX equations system specifically designed for the Greek language. We propose an end-to-end system that harnesses the power of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP) techniques to enable users to verbally dictate mathematical expressions and equations in natural language, which are subsequently converted into LaTeX format. We present the architecture and design principles of our system, highlighting key components such as the ASR engine, the LLM-based prompt-driven equations generation mechanism, as well as the application of a custom evaluation metric employed throughout the development process. We have made our system open source and available at https://github.com/magcil/greek-speech-to-math.},
    booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence},
    articleno = {19},
    numpages = {4},
    keywords = {Automatic Speech Recognition, Natural Language Processing, Accessibility, Equations},
    abbr = {SETN 2024},
    annotation={Work done while a member of the Computational Intelligence Laboratory of NCSR Demokritos},
    selected={True},
    location={Piraeus, Greece},
    google_scholar_id={Tyk-4Ss8FVUC},
    additional_info={. Our code is available [here](https://github.com/magcil/greek-speech-to-math)}
  }

  @inproceedings{DBLP:conf/clef/KaliosisMCPA23,
  abbr={CLEF 2023},
  author={Panagiotis Kaliosis and Georgios Moschovis and Foivos Charalampakos and John Pavlopoulos and Ion Androutsopoulos},
  title={AUEB NLP Group at ImageCLEFmedical Caption 2023},
  year={2023},
  cdate={1672531200000},
  pages={1524-1548},
  url={https://ceur-ws.org/Vol-3497/paper-126.pdf},
  booktitle={Conference and Labs of the Evaluation Forum (Working Notes)},
  crossref={conf/clef/2023w},
  abstract={This article describes the methods that the AUEB NLP Group experimented with during its participation in the 7th edition of the ImageCLEFmedical Caption sub-tasks, namely Concept Detection and Caption Prediction. The former intends to automatically classify biomedical images into a set of one or more tags based solely on the visual input, while the latter aims to generate a syntactically and semantically accurate diagnostic caption that addresses the medical conditions depicted on a given image. For the Concept Detection sub-task, extending our previous work, we utilized a wide range of Convolutional Neural Network encoders followed by a Feed-Forward Neural Network, both in a single-task and a multi-task fashion, as well as combined with a contrastive learning approach. Our methods concerning the Caption Prediction sub-task are influenced by both our previous work and recent progress in Natural Language Processing (NLP) methods. Our two base systems use CNN-RNN and Transformer-to-Transformer encoder-decoder architectures, respectively. Additionally, we experimented with a Transformer-based denoising component, which was trained to reformulate the generated captions in a more syntactically coherent and medically accurate way. Our group ranked 1st in Concept Detection and 3rd in Caption Prediction.},
  location={Thessaloniki, Greece},
  annotation={Work done while a member of the the NLP Group of AUEB},
  google_scholar_id={2osOgNQ5qMEC},
  selected={True}

}


@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein*†, A. and Podolsky*, B. and Rosen*, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.},
  location={New Jersey},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  altmetric={248277},
  dimensions={true},
  google_scholar_id={qyhmnyLat1gC},
  video={https://www.youtube-nocookie.com/embed/aqz-KE-bpKQ},
  additional_info={. *More Information* can be [found here](https://github.com/alshedivat/al-folio/)},
  annotation={* Example use of superscripts<br>† Albert Einstein},
  selected={true},
  inspirehep_id = {3255}
}